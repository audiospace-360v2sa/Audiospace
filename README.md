# AudioSpace: Generating Spatial Audio from 360-Degree Video  

âœ¨ğŸ”Š Transform your 360-degree videos into immersive spatial audio! ğŸŒğŸ¶  

<img src="assets/figure1-a.png" width="45%"> <img src="assets/figure1-b.png" width="45%">  

PyTorch Implementation of **AudioSpace**, a model for generating spatial audio from 360-degree videos.  

Due to anonymization requirements, ##**our model will be available on Hugging Face soon.**  

---

## ğŸ¬ Quick Start  
We provide an example of how you can perform inference using AudioSpace.  

### ğŸƒ Inference with Pretrained Model  
To run inference, follow these steps:  

1ï¸âƒ£ **Navigate to the root directory.** ğŸ“‚  
2ï¸âƒ£ **Create the Inference Environment.**  

To set up the environment, ensure you have **Python >= 3.8.20** installed. Then, run the following commands:  

```bash
pip install -r requirements.txt
pip install git+https://github.com/patrick-kidger/torchcubicspline.git
```
 
3ï¸âƒ£ **Run inference with the provided script:**  
   ```bash
   bash demo.sh video_path checkpoint_dir cuda_id
   ```  
ğŸ’¡ *You can also modify `demo.sh` to change the output directory.* The `cases` folder contains some sample 360-degree videos in the equirectangular formatâ€”make sure your videos follow the same format! ğŸ¥âœ¨  

---

## ğŸ“Š Evaluation  
To evaluate generated audio, follow these steps:  

1ï¸âƒ£ **Navigate to** `metrics/evaluate`. ğŸ“‚  
2ï¸âƒ£ **Create the evaluation environment:**  
    To set up the environment, ensure you have **Python >= 3.10.15** installed. Then, run the following commands: 
   ```bash
   pip install -r requirements.txt
   ```  
3ï¸âƒ£ **Modify `eval.sh`** to specify:  
   - ğŸ§ The path to real (ground truth) audio files  
   - ğŸµ The path to generated audio files  
   - ğŸ“œ The text file containing the names of the audio files to be compared  

Before running `eval.sh`, make sure to fill in these variables inside the script:  
```bash
reference_path=""    # Path to ground truth audios  
split_path="samples.txt"  # Path to split txt  
result_path=""    # Path to save results  
```  

4ï¸âƒ£ **Run evaluation:**  
   ```bash
   bash eval.sh cuda_id
   ```  

âœ¨ **Done!** ğŸ‰ Now you can compare the generated spatial audio with the ground truth! ğŸš€ğŸ”  

---

ğŸ’¡ *Have fun experimenting with AudioSpace! ğŸ› ï¸ğŸ’–*

## References and Licensing 

The model and inference components are adapted from [Stable Audio Tools](https://github.com/Stability-AI/stable-audio-tools). The corresponding license can be found in `audiospace/LICENSE`. Stable Audio Tools also includes dependencies that require specific licenses, which are available in the `audiospace/LICENSES` directory.  

The **metrics** components are adapted from [PaSST](https://github.com/kkoutini/PaSST) and [OpenL3](https://github.com/marl/openl3). Their respective licenses can be found in `metrics/LICENSES`. For a detailed breakdown, refer to `metrics/LICENSES/README.md`.  

## Acknowledgments  
We would like to express our gratitude to the authors and contributors of **Stable Audio Tools, PaSST, and OpenL3** for their invaluable work, which has greatly influenced and supported this project. Their contributions to the open-source community have been instrumental in advancing audio-related research and applications.  

